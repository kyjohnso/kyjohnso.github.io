<!DOCTYPE html>
<html lang="en">
<head>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        
        .centered-text {
            text-align: center;
            width: 80%;
            margin: 0 auto;
        }

        .centered-text h2 {
            font-weight: bold;
        }

        .centered-text h3 {
            font-weight: bold;
        }
        .centered-text p {
            text-align: justify;
        }

        /* Set a specific width and a max width for images */
        .media-image {
            width: 800px; /* Set specific width */
            max-width: 100%; /* Ensure responsiveness */
            height: auto;
            display: block;
            margin: 0 auto;
        }

        /* Wrapper for responsive 16:9 iframe aspect ratio */
        .video-wrapper {
            position: relative;
            width: 800px; /* Set specific width */
            max-width: 100%; /* Ensure responsiveness */
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin: 0 auto;
            overflow: hidden;
            background: #000;
        }

        .video-wrapper iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
        }
    </style>
</head>
<body>
    <h1>kyjohnso dot com</h1>
    <p>This is my space for collecting some interesting projects I have worked on. Enjoy - </p><br><br>

    <div class="centered-text">
        <h2>Regolith Voxel</h2>
        <p>
            <a href="https://github.com/kyjohnso/regolith_voxel" target="_blank" rel="noopener noreferrer">Regolith Voxel</a> is
            an experimental mining simulation game that serves as a testbed for reinforcement learning research. The project explores
            how RL agents can learn to manage complex resource extraction operations through indirect control of semi-autonomous mining equipment.
        </p>
        <img src="./docs/assets/images/regolith_voxel_screenshot.png" alt="Regolith Voxel Screenshot" class="media-image" loading="lazy">
        <p>
            The core concept investigates <strong>indirect control paradigms</strong> where players provide high-level strategic direction
            rather than direct commands. Each piece of mining equipment operates autonomously, making local decisions based on its type,
            position, and observations of the mineral-rich environment. This creates emergent behavior as multiple agents coordinate
            their activities—samplers scout for resources, miners extract materials at different depths, refiners process raw ore,
            and transport units move goods between sites.
        </p>
        <p>
            What makes this particularly interesting for RL research is the <strong>network effects</strong> between agents. A single
            agent's optimal strategy depends heavily on what other agents are doing: transport units need miners to produce ore,
            refiners need raw materials delivered, and efficient sampling can guide mining operations to high-value deposits. This
            multi-agent coordination challenge, combined with the hierarchical organization of equipment and the spatial dynamics
            of resource management, provides a rich environment for exploring how reinforcement learning agents can learn to cooperate
            and adapt in complex systems.
        </p>
        <p>
            Built with Rust and the Bevy game engine, the project features procedurally generated mineral maps, a drag-and-drop
            equipment hierarchy system, and a modular architecture designed to integrate RL training loops. Future work will focus
            on implementing policy networks for autonomous equipment behavior and experimenting with reward structures that encourage
            efficient multi-agent coordination.
        </p>
    </div>

    <div class="centered-text">
        <h2>ThreeJS Map</h2>
        <p>This is the current state of a very simple map and terrain visualization in threejs. It will accept latitude and longitude
            and zoom level and render a single tile in the canvas. The terrain data is provided by 
            <a href="https://s3.amazonaws.com/elevation-tiles-prod/index.html" target="_blank" rel="noopener noreferrer">Mapzen</a> documented at
            <a href="https://github.com/tilezen/joerd/tree/master/docs"  target="_blank" rel="noopener noreferrer">tilezen</a> under the 
            <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">
                Creative Commons BY 4.0</a> license. 
            The topo map is provided by 
            <a href="https://opentopomap.org/" target="_blank" rel="noopener noreferrer">Open Topo Map</a> under the 
            <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" target="_blank" rel="noopener noreferrer">Creative Commons BY-SA 4.0</a> license.   
            My code can be found at 
            <a href="https://github.com/kyjohnso/three_map"  target="_blank" rel="noopener noreferrer">ThreeJS Map</a>. 
            Try playing around with the "random location" button.
          
        </p>

        <iframe data-src="three_map/index.html" width="80%" height="600px" style="border:none;" loading="lazy">
            <!-- Optional fallback content if iframe is not supported -->
            <p>Your browser does not support iframes.</p>
        </iframe>  
    </div>

    <div class="centered-text">

        <h2>Gaussian Splatting</h2>
        <p>This is my first experiment with a technique called Gaussian Splatting. Hopefully much more to come in the future; but for now,
            I was able to create the images below and the render at <a href="./school_splat/"  target="_blank" rel="noopener noreferrer">School Splat</a> with completely open source code: 
            <a href="https://www.blender.org/" target="_blank" rel="noopener noreferrer">Blender</a> =>
            <a href="https://colmap.github.io/" target="_blank" rel="noopener noreferrer">Colmap</a> =>        
            <a href="https://github.com/graphdeco-inria/gaussian-splatting" target="_blank" rel="noopener noreferrer">Gaussian Splatting</a> =>
            <a href="https://github.com/antimatter15/splat" target="_blank" rel="noopener noreferrer">Antimatter15's WebGL Splat Renderer</a>.
            
        </p>
        <!-- <img src="docs/assets/images/school1.png" alt="School Splat Rendered Image 1" class="media-image" loading="lazy"> -->
        <img src="docs/assets/images/school2.png" alt="School Splat Rendered Image 2" class="media-image" loading="lazy">
        <!-- <img src="docs/assets/images/school3.png" alt="School Splat Rendered Image 3" class="media-image" loading="lazy"> -->
        <img src="docs/assets/images/school4.png" alt="School Splat Rendered Image 4" class="media-image" loading="lazy">
        <img src="docs/assets/images/school5.png" alt="School Splat Rendered Image 5" class="media-image" loading="lazy">
        <!-- <img src="docs/assets/images/school6.png" alt="School Splat Rendered Image 6" class="media-image" loading="lazy"> -->

    </div>

    <div class="centered-text">

        <h2>AI Alignment Fundamentals Course</h2>
        <p> In February-2025, I completed Blue Dot Impact's <a href="https://aisafetyfundamentals.com/alignment/" target="_blank" rel="noopener noreferrer">AI Alignment course</a>. 
            In this course we are exposed to a wide variety of AI alignment techniques, ideas, and research approaches. In the final third of the course, I completed
            a project in Computable Policy and World Model integration. You can find the post for this project at 
            <a href="https://hylaeansea.org/blog/2025/02/01/World-Models-and-Language-Models.html"target="_blank" rel="noopener noreferrer">Hylaean Sea</a>. As with all of these projects (it seems) I plan to develop this
            concept more in the future. 
            <img src="./docs/assets/images/New_Knowledge.png" alt="New Knowledge" class="media-image" loading="lazy">
        </p>
        <p> I have also been engaging a local chat group on some high level questions about AI Alignment Philosophy and will be including those prompts here as well.</p>
        <h3>Distribution of AI</h3>
        
        <img src="./docs/assets/images/DALL·E_2025-01-25-tippling_point_s.webp" alt="Tipping Point" class="media-image" loading="lazy">
        <p>A thought I have been pondering recently: assuming the economic, leisure, and cultural benefits of AI are very large (they may not be but lets assume they are), what are the major tipping points or key factors that will determine if these AI resources are concentrated in the hands of a few AI elite, or the AI upper class; or are spread out in society for everyone's benefit? Is it education, fear, use in businesses, open source models, or something else that could make the difference between it being available to the masses vs locked away?</p>
        
        <h3>AI Unlearning</h3>
        
        <img src="./docs/assets/images/DALLE_image_of_AI_removing_knowledge_of_another_AI.webp" alt="Knowledge Removal" class="media-image" loading="lazy">
        <p>There is significant research into AI alignment at the fine-tuning stage. Several techniques attempt to make LLMs forget, or unlearn harmful 
            knowledge that the base model acquired in the training phase on bulk internet data. We try to make it "unlearn" how to make a bomb, or how to 
            make a nerve-agent for example. Is this wise? Don't we want LLMs to have harmful knowledge so that they can recognize it, as it comes up in 
            prompts? Does this have parallels for human knowledge? Is there "harmful knowledge" that we should try to unlearn as a society?</p>


        <h3>Bootstrap Training</h3>
        <p>If we need humans to generate training or fine tuning data to train advanced AI's can they ever become more capable than humans? 
            Can a human train something more cognitively capable than itself? If so, how do we make sure that these more capable AIs are aligned with our goals?
        <img src="./docs/assets/images/DALLE-multiple_ais.jpeg" alt="multiple ais" class="media-image" loading="lazy">
        </p>

    </div>

    <div class="centered-text">
        <h2>Blender Con 2024</h2>
        <p>I was fortunate to have my talk selected for Blender Con 2024. I expected to have just a 20 minute talk but I was slotted for a live 50-minute walkthrough of some of the projects that appear on this page.
            I began with a walkthrough of the geometry nodes approach to modeling parametric surfaces, then demonstrated how to use modifiers and scripting to model the 3D dodecahedron moon.
        </p>
        <img src="./docs/assets/images/segmented_moon_rendered.png" alt="Rendered Moon" class="media-image" loading="lazy"><br>
        <img src="./docs/assets/images/segmented_moon_printed.jpg" alt="Printed Moon" class="media-image" loading="lazy"><br>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/m55iR8__tZc?si=MBJD2U96D7OhYiKs" allowfullscreen></iframe>
        </div><br>
        <p> The files referenced in the video can be found at <a href="./bcon24/index.html" target="_blank" rel="noopener noreferrer">blender con 24 files</a>.</p>
        <p> All 12 moon .stl files are available under a Creative Commons license at <a href="https://www.printables.com/model/1059470-dodecahedron-magnetic-moon" target="_blank" rel="noopener noreferrer">Printables.com</a>.</p>
    </div>
    <div class="centered-text">
        <h2>Ideal Gas Simulation</h2>
        <p>My daughter was studying the states of matter in her science class and she had a fairly good grasp of the molecular (non-quantum) mental model of matter and how it translates
            to the physical properties of the material in different states. She, however had a bit of trouble with visualizing why an ideal gas will take the shape of its container. After much
            explaining, pretending we were molecules and dancing around, and throwing fruit off the walls and ceiling of the kitchen, we decided to have chatgpt help us visualize this in 
            a web simulation. The simulation was written in threejs and can be found at <a href="./ideal_gas/index.html" target="_blank" rel="noopener noreferrer">Ideal Gas Simulation</a>
        </p>
        <!-- The iframe is now set to load lazily via Intersection Observer -->
        <iframe data-src="ideal_gas/index.html" width="80%" height="600px" style="border:none;" loading="lazy">
            <!-- Optional fallback content if iframe is not supported -->
            <p>Your browser does not support iframes.</p>
        </iframe>
    </div>
    <div class="centered-text">
        <h2>Dodecahedron Moon</h2>
        <p>This is a 3D printed design of the moon (real topography from 
        <a href="https://svs.gsfc.nasa.gov/cgi-bin/details.cgi?aid=4720" target="_blank" rel="noopener noreferrer">NASA Moon Kit</a>) broken 
        into the curved hexagonal faces of a dodecahedron. The whole design is held together by
        force-fit magnets. The full 12 STL files will be posted soon at Printables.</p>
        <img src="./docs/assets/images/dodecamoon_1.jpg" alt="Part 1 Printed" class="media-image" loading="lazy"><br>
    </div>
    <div class="centered-text">
        <h2>Frequency Difference</h2>
        <p>
            An exploration of the intersection of quadrics. Frequency difference (doppler difference) from a location to two different moving sensors
            can be modeled as two intersecting cones with related cone angles. Set to the music of Hans Zimmer, it can be quite mesmerizing.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/x51qyfJ1XNM" allowfullscreen></iframe>
        </div><br>
    </div>

    <div class="centered-text">
        <h2>CGI Sandbox</h2>
        <p>
            An exploration of CGI with drone footage. So far done only with Blender motion tracking. Hopefully evolving over the coming months. Drone footage courtesy of my buddy Greg.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/yK5mDySToXA" allowfullscreen></iframe>
        </div><br>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/AZqEn46I-dM" allowfullscreen></iframe>
        </div><br>
    </div>
    <div class="centered-text">
        <h2>Intersecting Hyperboloids</h2>
        <p>
            A visualization of two intersecting hyperboloids with one of their foci shared. It is interesting to note that the intersection pencil will always intersect the plane formed by the three foci at a right angle.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/8C2cLSR_49o" allowfullscreen></iframe>
        </div><br>
    </div>
    <div class="centered-text">
        <h2>Time Difference Visualization</h2>
        <p>
            A visualization of a time difference surface made in Blender.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/cLdlvLBH_sY" allowfullscreen></iframe>
        </div><br>
    </div>
    <div class="centered-text">
        <h2>Alexa-Enabled Maze Solving Robot</h2>
        <p>
            <a href="https://www.hackster.io/kyle22/dwr-an-alexa-voice-enabled-maze-solving-lego-robot-405dbf" target="_blank" rel="noopener noreferrer">Alexa Enabled Maze Solving Robot</a> - An entry into a contest to link Amazon Alexa Gadgets to Lego Mindstorms. I made a robot that you gave instructions to via Alexa to help it solve a Lego maze.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/NFJVlPlYQhk" allowfullscreen></iframe>
        </div><br>
    </div>
    
    <!-- Script for lazy loading the ideal_gas iframe -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        const lazyIframes = document.querySelectorAll('iframe[data-src]');
        
        const loadIframe = iframe => {
            iframe.src = iframe.getAttribute('data-src');
            iframe.removeAttribute('data-src');
        };
        
        if ('IntersectionObserver' in window) {
            let observer = new IntersectionObserver((entries, observer) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        loadIframe(entry.target);
                        observer.unobserve(entry.target);
                    }
                });
            });
            lazyIframes.forEach(iframe => {
                observer.observe(iframe);
            });
        } else {
            // Fallback for browsers that do not support IntersectionObserver
            lazyIframes.forEach(iframe => {
                loadIframe(iframe);
            });
        }
    });
    </script>
</body>
</html>
