<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Kyle Johnson's portfolio showcasing projects in 3D graphics, Gaussian Splatting, Blender, AI alignment, and interactive simulations.">
    <meta name="author" content="Kyle Johnson">
    <meta property="og:title" content="kyjohnso dot com">
    <meta property="og:description" content="Portfolio of 3D graphics, Gaussian Splatting, and interactive visualization projects">
    <meta property="og:type" content="website">
    <title>Kyle Johnson - Portfolio</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>kyjohnso dot com</h1>
    <p>This is my space for collecting some interesting projects I have worked on. Enjoy - </p>

    <div class="centered-text" id="strangerthingswall">
        <h2>Stranger Things Wall</h2>
        <h3>Raspberry Pi, LEDs, and Claude</h3>
        <picture>
            <source srcset="./docs/assets/images/st_season5_with_wall.jpg" type="image/webp">
            <img src="./docs/assets/images/st_season5_with_wall.jpg" alt="Stranger Things Season 5" class="media-image" loading="lazy">
        </picture>
        <p>
            I originally built this project in May 2022 for Stranger Things Season 4, recreating the iconic scene where Will
            communicates from the Upside Down through Christmas lights. Initially, I planned to integrate voice activation using
            Amazon's Alexa Gadgets API (similar to my <a href="#alexa-maze">Alexa-enabled Lego Mindstorms maze solver</a> below),
            but Amazon discontinued the API before I could complete the integration. I was left with a static message generator
            that could display pre-programmed text for Halloween, the Season 4 premiere, and secret messages to my kids.
        </p>

        <img src="./docs/assets/images/will_byers_demo.gif" alt="Will Byers LED Chat Demo" class="media-image" loading="lazy">

        <p>
            With Season 5 here and the massive improvements in LLMs and speech-to-text technology, I decided to revisit
            the project in 2025. Now it uses <a href="https://alphacephei.com/vosk/" target="_blank" rel="noopener noreferrer">Vosk</a>
            for custom wake-word detection, <a href="https://github.com/openai/whisper" target="_blank" rel="noopener noreferrer">Whisper</a>
            for speech-to-text, and the <a href="https://github.com/anthropics/anthropic-sdk-python" target="_blank" rel="noopener noreferrer">Anthropic SDK</a>
            for generating Will's responses. The result realizes my original 2022 vision—you can now talk to Will in the Upside Down,
            and he replies through the Christmas Light Alphabet!
        </p>

        <p>
            The hardware consists of a Raspberry Pi 4B controlling a strand of 100 individually addressable WS281x LEDs connected
            to GPIO 18. The system runs on a 12V power supply for the LEDs (5V is also available) and a separate USB power supply for the Pi.
            The major bill of materials includes:
        </p>
        <ul style="text-align: left; max-width: 600px; margin: 0 auto 2rem;">
            <li>Raspberry Pi 4B (any Pi with GPIO works)</li>
            <li>12V WS281x LED strip (100 LEDs)</li>
            <li>12V power supply for LEDs</li>
            <li>Logic level converter (3.3V to 5V)</li>
        </ul>

        <picture>
            <source srcset="./docs/assets/images/engineering_notebook_entry.webp" type="image/webp">
            <img src="./docs/assets/images/engineering_notebook_entry.png" alt="Engineering Notebook Entry from May 2022" class="media-image" loading="lazy">
        </picture>

        <picture>
            <source srcset="./docs/assets/images/breadboard.webp" type="image/webp">
            <img src="./docs/assets/images/breadboard.jpg" alt="Prototype breadboard controlling the LED display" class="media-image" loading="lazy">
        </picture>

        <p>
            To create the original display in 2022, my daughter hand-painted all 26 letters of the alphabet on cardboard, we cut them all out, then hung them on the
            wall. Since we can control any light, I held off on creating a letter-to-light address map until after everything was hung.
            In this way I could illuminate the lights individually with the code, and then note which address was closest to which letter. In the New 
            version there is a script that assists with making this mapping.
        </p>

        <picture>
            <source srcset="./docs/assets/images/painting_the_alphabet.webp" type="image/webp">
            <img src="./docs/assets/images/painting_the_alphabet.jpg" alt="Hand-painting letters on paper" class="media-image" loading="lazy">
        </picture>

        <picture>
            <source srcset="./docs/assets/images/full_color_wall.webp" type="image/webp">
            <img src="./docs/assets/images/full_color_wall.jpg" alt="Completed LED installation with all letters lit" class="media-image" loading="lazy">
        </picture>

        <p>
            Between 2022 and 2025, we had our flat painted, so rather than painting letters on the wall again, I decided to make a section
            of the wall out of thin MDF. This gave me the opportunity for a major upgrade—70's era floral wallpaper as a background for the
            lettering.
        </p>

        <picture>
            <source srcset="./docs/assets/images/wall_paper_closeup.webp" type="image/webp">
            <img src="./docs/assets/images/wall_paper_closeup.jpg" alt="Self-adhesive 70s floral wallpaper adds authenticity" class="media-image" loading="lazy">
        </picture>

        <p>
            If you use the keyboard-triggered voice chat, you can even sync with Season 1 Episode 3! Watch the demo below:
        </p>

        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/hM04iaqw8V4" allowfullscreen></iframe>
        </div><br>

        <p>
            The updated project is available at <a href="https://github.com/kyjohnso/will_byers" target="_blank" rel="noopener noreferrer">github.com/kyjohnso/will_byers</a>
            and includes multiple interaction modes: a static message player for pre-programmed text, an interactive chat mode where
            Claude AI responds in character as Will Byers, and wake-word voice activation where you can speak directly to Will.
            The AI integration uses the Anthropic API to generate contextual responses, with each letter flashing in sequence on the
            wall—complete with dramatic pauses for spaces, just like in the show.
        </p>
        <p>
            I would LOVE to see a bunch of posts of you all recreating this scene in the run-up to the finale—this is going to be a good one!
        </p>


    </div>

    <div class="centered-text">
        <h2>Regolith Voxel</h2>
        <p>
            <a href="https://github.com/kyjohnso/regolith_voxel" target="_blank" rel="noopener noreferrer">Regolith Voxel</a> is
            an experimental mining simulation game that serves as a testbed for reinforcement learning research. The project explores
            how RL agents can learn to manage complex resource extraction operations through indirect control of semi-autonomous mining equipment.
        </p>
        <picture>
            <source srcset="./docs/assets/images/regolith_voxel_screenshot.webp" type="image/webp">
            <img src="./docs/assets/images/regolith_voxel_screenshot.png" alt="Regolith Voxel Screenshot" class="media-image" loading="lazy">
        </picture>
        <p>
            The core concept investigates <strong>indirect control paradigms</strong> where players provide high-level strategic direction
            rather than direct commands. Each piece of mining equipment operates autonomously, making local decisions based on its type,
            position, and observations of the mineral-rich environment. This creates emergent behavior as multiple agents coordinate
            their activities—samplers scout for resources, miners extract materials at different depths, refiners process raw ore,
            and transport units move goods between sites.
        </p>
        <p>
            What makes this particularly interesting for RL research is the <strong>network effects</strong> between agents. A single
            agent's optimal strategy depends heavily on what other agents are doing: transport units need miners to produce ore,
            refiners need raw materials delivered, and efficient sampling can guide mining operations to high-value deposits. This
            multi-agent coordination challenge, combined with the hierarchical organization of equipment and the spatial dynamics
            of resource management, provides a rich environment for exploring how reinforcement learning agents can learn to cooperate
            and adapt in complex systems.
        </p>
        <p>
            Built with Rust and the Bevy game engine, the project features procedurally generated mineral maps, a drag-and-drop
            equipment hierarchy system, and a modular architecture designed to integrate RL training loops. Future work will focus
            on implementing policy networks for autonomous equipment behavior and experimenting with reward structures that encourage
            efficient multi-agent coordination.
        </p>
    </div>

    <div class="centered-text">
        <h2>SkySplat: 3D Gaussian Splatting Toolkit</h2>
        <p>
            <a href="https://skysplat.org" target="_blank" rel="noopener noreferrer">SkySplat</a> is a comprehensive Blender addon
            that streamlines the creation of photorealistic 3D Gaussian Splats from drone footage and aerial photography. The project
            bridges the gap between raw video capture and finished 3D scenes by integrating structure-from-motion (COLMAP),
            Gaussian Splatting training, and Blender's powerful 3D workflow into a single cohesive toolkit.
        </p>
        <picture>
            <source srcset="./docs/assets/images/puente_nuevo_bridge.webp" type="image/webp">
            <img src="./docs/assets/images/puente_nuevo_bridge.png" alt="3D Gaussian Splat of Puente Nuevo Bridge" class="media-image" loading="lazy">
        </picture>
        <p>
            What sets SkySplat apart is its focus on <strong>coordinate system transformation and spatial integration</strong>. Rather
            than treating reconstructed scenes as isolated assets, the addon allows you to rotate, scale, and position COLMAP point
            clouds within Blender's native coordinate space before Gaussian Splatting training. This means you can align reconstructions
            with reference imagery, combine multiple splats, or integrate traditional 3D models into photorealistic captures—all while
            maintaining accurate spatial relationships.
        </p>
        <picture>
            <source srcset="./docs/assets/images/3dgs_model_loaded_5.webp" type="image/webp">
            <img src="./docs/assets/images/3dgs_model_loaded_5.png" alt="SkySplat Workflow in Blender" class="media-image" loading="lazy">
        </picture>
        <p>
            The complete workflow runs entirely within Blender: import video, extract frames with metadata, run COLMAP reconstruction,
            transform and orient the scene, train Gaussian Splats with the bundled Brush renderer, and load the results directly into
            your viewport. The addon bundles pre-compiled Gaussian Splatting binaries for all platforms and provides real-time training
            visualization, making advanced 3D capture accessible to artists and researchers without complex dependency management.
        </p>
        <p>
            I presented this project at BlenderCon 2025, demonstrating workflows for architectural visualization, terrain reconstruction,
            and mixed CGI/photogrammetry compositions. The project is open source on GitHub at
            <a href="https://github.com/kyjohnso/skysplat_blender" target="_blank" rel="noopener noreferrer">kyjohnso/skysplat_blender</a>,
            with comprehensive documentation and example workflows at <a href="https://skysplat.org" target="_blank" rel="noopener noreferrer">skysplat.org</a>.
        </p>
    </div>

    <div class="centered-text">
        <h2>ThreeJS Map</h2>
        <p>This is the current state of a very simple map and terrain visualization in threejs. It will accept latitude and longitude
            and zoom level and render a single tile in the canvas. The terrain data is provided by 
            <a href="https://s3.amazonaws.com/elevation-tiles-prod/index.html" target="_blank" rel="noopener noreferrer">Mapzen</a> documented at
            <a href="https://github.com/tilezen/joerd/tree/master/docs"  target="_blank" rel="noopener noreferrer">tilezen</a> under the 
            <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">
                Creative Commons BY 4.0</a> license. 
            The topo map is provided by 
            <a href="https://opentopomap.org/" target="_blank" rel="noopener noreferrer">Open Topo Map</a> under the 
            <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" target="_blank" rel="noopener noreferrer">Creative Commons BY-SA 4.0</a> license.   
            My code can be found at 
            <a href="https://github.com/kyjohnso/three_map"  target="_blank" rel="noopener noreferrer">ThreeJS Map</a>. 
            Try playing around with the "random location" button.
          
        </p>

        <iframe data-src="three_map/index.html" width="80%" height="600px" style="border:none;" loading="lazy">
            <!-- Optional fallback content if iframe is not supported -->
            <p>Your browser does not support iframes.</p>
        </iframe>  
    </div>

    <div class="centered-text">

        <h2>Gaussian Splatting</h2>
        <p>This is my first experiment with a technique called Gaussian Splatting. Hopefully much more to come in the future; but for now,
            I was able to create the images below and the render at <a href="./school_splat/"  target="_blank" rel="noopener noreferrer">School Splat</a> with completely open source code: 
            <a href="https://www.blender.org/" target="_blank" rel="noopener noreferrer">Blender</a> =>
            <a href="https://colmap.github.io/" target="_blank" rel="noopener noreferrer">Colmap</a> =>        
            <a href="https://github.com/graphdeco-inria/gaussian-splatting" target="_blank" rel="noopener noreferrer">Gaussian Splatting</a> =>
            <a href="https://github.com/antimatter15/splat" target="_blank" rel="noopener noreferrer">Antimatter15's WebGL Splat Renderer</a>.
            
        </p>
        <!-- <img src="docs/assets/images/school1.png" alt="School Splat Rendered Image 1" class="media-image" loading="lazy"> -->
        <picture>
            <source srcset="docs/assets/images/school2.webp" type="image/webp">
            <img src="docs/assets/images/school2.png" alt="School Splat Rendered Image 2" class="media-image" loading="lazy">
        </picture>
        <!-- <img src="docs/assets/images/school3.png" alt="School Splat Rendered Image 3" class="media-image" loading="lazy"> -->
        <picture>
            <source srcset="docs/assets/images/school4.webp" type="image/webp">
            <img src="docs/assets/images/school4.png" alt="School Splat Rendered Image 4" class="media-image" loading="lazy">
        </picture>
        <picture>
            <source srcset="docs/assets/images/school5.webp" type="image/webp">
            <img src="docs/assets/images/school5.png" alt="School Splat Rendered Image 5" class="media-image" loading="lazy">
        </picture>
        <!-- <img src="docs/assets/images/school6.png" alt="School Splat Rendered Image 6" class="media-image" loading="lazy"> -->

    </div>

    <div class="centered-text">

        <h2>AI Alignment Fundamentals Course</h2>
        <p> In February-2025, I completed Blue Dot Impact's <a href="https://aisafetyfundamentals.com/alignment/" target="_blank" rel="noopener noreferrer">AI Alignment course</a>. 
            In this course we are exposed to a wide variety of AI alignment techniques, ideas, and research approaches. In the final third of the course, I completed
            a project in Computable Policy and World Model integration. You can find the post for this project at 
            <a href="https://hylaeansea.org/blog/2025/02/01/World-Models-and-Language-Models.html"target="_blank" rel="noopener noreferrer">Hylaean Sea</a>. As with all of these projects (it seems) I plan to develop this
            concept more in the future. 
            <img src="./docs/assets/images/New_Knowledge.png" alt="New Knowledge" class="media-image" loading="lazy">
        </p>
        <p> I have also been engaging a local chat group on some high level questions about AI Alignment Philosophy and will be including those prompts here as well.</p>
        <h3>Distribution of AI</h3>
        
        <img src="./docs/assets/images/DALL·E_2025-01-25-tippling_point_s.webp" alt="Tipping Point" class="media-image" loading="lazy">
        <p>A thought I have been pondering recently: assuming the economic, leisure, and cultural benefits of AI are very large (they may not be but lets assume they are), what are the major tipping points or key factors that will determine if these AI resources are concentrated in the hands of a few AI elite, or the AI upper class; or are spread out in society for everyone's benefit? Is it education, fear, use in businesses, open source models, or something else that could make the difference between it being available to the masses vs locked away?</p>
        
        <h3>AI Unlearning</h3>
        
        <img src="./docs/assets/images/DALLE_image_of_AI_removing_knowledge_of_another_AI.webp" alt="Knowledge Removal" class="media-image" loading="lazy">
        <p>There is significant research into AI alignment at the fine-tuning stage. Several techniques attempt to make LLMs forget, or unlearn harmful 
            knowledge that the base model acquired in the training phase on bulk internet data. We try to make it "unlearn" how to make a bomb, or how to 
            make a nerve-agent for example. Is this wise? Don't we want LLMs to have harmful knowledge so that they can recognize it, as it comes up in 
            prompts? Does this have parallels for human knowledge? Is there "harmful knowledge" that we should try to unlearn as a society?</p>


        <h3>Bootstrap Training</h3>
        <p>If we need humans to generate training or fine tuning data to train advanced AI's can they ever become more capable than humans? 
            Can a human train something more cognitively capable than itself? If so, how do we make sure that these more capable AIs are aligned with our goals?
        <img src="./docs/assets/images/DALLE-multiple_ais.jpeg" alt="multiple ais" class="media-image" loading="lazy">
        </p>

    </div>

    <div class="centered-text">
        <h2>Blender Con 2024</h2>
        <p>I was fortunate to have my talk selected for Blender Con 2024. I expected to have just a 20 minute talk but I was slotted for a live 50-minute walkthrough of some of the projects that appear on this page.
            I began with a walkthrough of the geometry nodes approach to modeling parametric surfaces, then demonstrated how to use modifiers and scripting to model the 3D dodecahedron moon.
        </p>
        <picture>
            <source srcset="./docs/assets/images/segmented_moon_rendered.webp" type="image/webp">
            <img src="./docs/assets/images/segmented_moon_rendered.png" alt="Rendered Moon" class="media-image" loading="lazy">
        </picture><br>
        <picture>
            <source srcset="./docs/assets/images/segmented_moon_printed.webp" type="image/webp">
            <img src="./docs/assets/images/segmented_moon_printed.jpg" alt="Printed Moon" class="media-image" loading="lazy">
        </picture><br>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/m55iR8__tZc?si=MBJD2U96D7OhYiKs" allowfullscreen></iframe>
        </div><br>
        <p> The files referenced in the video can be found at <a href="./bcon24/index.html" target="_blank" rel="noopener noreferrer">blender con 24 files</a>.</p>
        <p> All 12 moon .stl files are available under a Creative Commons license at <a href="https://www.printables.com/model/1059470-dodecahedron-magnetic-moon" target="_blank" rel="noopener noreferrer">Printables.com</a>.</p>
    </div>
    <div class="centered-text">
        <h2>Ideal Gas Simulation</h2>
        <p>My daughter was studying the states of matter in her science class and she had a fairly good grasp of the molecular (non-quantum) mental model of matter and how it translates
            to the physical properties of the material in different states. She, however had a bit of trouble with visualizing why an ideal gas will take the shape of its container. After much
            explaining, pretending we were molecules and dancing around, and throwing fruit off the walls and ceiling of the kitchen, we decided to have chatgpt help us visualize this in 
            a web simulation. The simulation was written in threejs and can be found at <a href="./ideal_gas/index.html" target="_blank" rel="noopener noreferrer">Ideal Gas Simulation</a>
        </p>
        <!-- The iframe is now set to load lazily via Intersection Observer -->
        <iframe data-src="ideal_gas/index.html" width="80%" height="600px" style="border:none;" loading="lazy">
            <!-- Optional fallback content if iframe is not supported -->
            <p>Your browser does not support iframes.</p>
        </iframe>
    </div>
    <div class="centered-text">
        <h2>Dodecahedron Moon</h2>
        <p>This is a 3D printed design of the moon (real topography from 
        <a href="https://svs.gsfc.nasa.gov/cgi-bin/details.cgi?aid=4720" target="_blank" rel="noopener noreferrer">NASA Moon Kit</a>) broken 
        into the curved hexagonal faces of a dodecahedron. The whole design is held together by
        force-fit magnets. The full 12 STL files will be posted soon at Printables.</p>
        <img src="./docs/assets/images/dodecamoon_1.jpg" alt="Part 1 Printed" class="media-image" loading="lazy"><br>
    </div>
    <div class="centered-text">
        <h2>Frequency Difference</h2>
        <p>
            An exploration of the intersection of quadrics. Frequency difference (doppler difference) from a location to two different moving sensors
            can be modeled as two intersecting cones with related cone angles. Set to the music of Hans Zimmer, it can be quite mesmerizing.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/x51qyfJ1XNM" allowfullscreen></iframe>
        </div><br>
    </div>

    <div class="centered-text">
        <h2>CGI Sandbox</h2>
        <p>
            An exploration of CGI with drone footage. So far done only with Blender motion tracking. Hopefully evolving over the coming months. Drone footage courtesy of my buddy Greg.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/yK5mDySToXA" allowfullscreen></iframe>
        </div><br>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/AZqEn46I-dM" allowfullscreen></iframe>
        </div><br>
    </div>
    <div class="centered-text">
        <h2>Intersecting Hyperboloids</h2>
        <p>
            A visualization of two intersecting hyperboloids with one of their foci shared. It is interesting to note that the intersection pencil will always intersect the plane formed by the three foci at a right angle.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/8C2cLSR_49o" allowfullscreen></iframe>
        </div><br>
    </div>
    <div class="centered-text">
        <h2>Time Difference Visualization</h2>
        <p>
            A visualization of a time difference surface made in Blender.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/cLdlvLBH_sY" allowfullscreen></iframe>
        </div><br>
    </div>
    <div class="centered-text" id="alexa-maze">
        <h2>Alexa-Enabled Maze Solving Robot</h2>
        <p>
            <a href="https://www.hackster.io/kyle22/dwr-an-alexa-voice-enabled-maze-solving-lego-robot-405dbf" target="_blank" rel="noopener noreferrer">Alexa Enabled Maze Solving Robot</a> - An entry into a contest to link Amazon Alexa Gadgets to Lego Mindstorms. I made a robot that you gave instructions to via Alexa to help it solve a Lego maze.
        </p>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/NFJVlPlYQhk" allowfullscreen></iframe>
        </div><br>
    </div>
    
    <!-- Script for lazy loading the ideal_gas iframe -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        const lazyIframes = document.querySelectorAll('iframe[data-src]');
        
        const loadIframe = iframe => {
            iframe.src = iframe.getAttribute('data-src');
            iframe.removeAttribute('data-src');
        };
        
        if ('IntersectionObserver' in window) {
            let observer = new IntersectionObserver((entries, observer) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        loadIframe(entry.target);
                        observer.unobserve(entry.target);
                    }
                });
            });
            lazyIframes.forEach(iframe => {
                observer.observe(iframe);
            });
        } else {
            // Fallback for browsers that do not support IntersectionObserver
            lazyIframes.forEach(iframe => {
                loadIframe(iframe);
            });
        }
    });
    </script>
</body>
</html>
